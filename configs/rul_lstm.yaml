model: lstm
layers:
  - units: 64
  - dropout: 0.2
optimizer: adam
learning_rate: 0.001
batch_size: 64
epochs: 150